# ğŸ” Adversarial Attacks on PathMNIST Using FGSM

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/)
[![Jupyter](https://img.shields.io/badge/Jupyter-Notebook-orange.svg)](https://jupyter.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

This project demonstrates adversarial attacks on medical image classification models using the **Fast Gradient Sign Method (FGSM)**, tested in both **white-box** and **black-box** settings with the **PathMNIST** dataset.

---

## ğŸ“š Table of Contents

- [ğŸ“ Project Structure](#-project-structure)
- [ğŸš€ How It Works](#-how-it-works)
  - [ğŸ”“ White-box Attack](#-white-box-attack)
  - [ğŸ•¶ï¸ Black-box Attack](#-black-box-attack)
- [ğŸ“Š Results](#-results)
- [ğŸ§ª Setup & Run](#-setup--run)
- [ğŸ“š Dataset](#-dataset)
- [ğŸ”­ Future Work](#-future-work)
- [ğŸ§‘â€ğŸ’» Author](#-author)

---

## ğŸ“ Project Structure

â”œâ”€â”€ attacks/ # Attack-specific modules
â”œâ”€â”€ data/ # Data preparation scripts
â”œâ”€â”€ models/ # Model definitions
â”œâ”€â”€ AdvFGSM_PathMNIST.ipynb # White-box FGSM attack notebook
â”œâ”€â”€ AdvBlackbox_pathmnist.ipynb # Black-box FGSM attack notebook
â”œâ”€â”€ conference_letter_report.docx # Report/summary document
â”œâ”€â”€ eval.py # Evaluation script
â”œâ”€â”€ model.py # Model architecture
â”œâ”€â”€ model_pathmnist.pth # Pretrained model
â”œâ”€â”€ requirements.txt # Python dependencies
â”œâ”€â”€ train.py # Model training
â”œâ”€â”€ utils.py # Helper functions
â””â”€â”€ README.md # Project documentation


---

## ğŸš€ How It Works

### ğŸ”“ White-box Attack

- Assumes full access to the model (weights & gradients).
- FGSM perturbs inputs in the direction of the gradient to mislead predictions.
- Evaluates the model's vulnerability to known-source attacks.

### ğŸ•¶ï¸ Black-box Attack

- No access to the target model.
- A **surrogate model** is trained to approximate the target.
- FGSM is applied on the surrogate, and adversarial examples are transferred.
- Tests **transferability** of adversarial attacks.

---

## ğŸ“Š Results

| Attack Type   | Clean Accuracy | Adversarial Accuracy | Epsilon (Îµ) |
|---------------|----------------|-----------------------|-------------|
| White-box     |  *e.g., 88%*    |  *e.g., 43%*           | 0.1         |
| Black-box     |  *e.g., 87%*    |  *e.g., 51%*           | 0.1         |

- Visual changes are minimal but enough to confuse the model.
- Even black-box attacks reduce accuracy significantly.

---

## ğŸ§ª Setup & Run

1. **Clone the repository**

```bash
git clone https://github.com/<your-username>/Adversarial-FGSM-PathMNIST.git
cd Adversarial-FGSM-PathMNIST
```

Install dependencies

```bash
pip install -r requirements.txt
```

Train the model

```bash
python train.py
```
Evaluate or run attacks
```bash
python eval.py
```
Or run Jupyter notebooks:
```bash
AdvFGSM_PathMNIST.ipynb
AdvBlackbox_pathmnist.ipynb
```
## ğŸ“š Dataset

Dataset: PathMNIST from the MedMNIST collection
Format: 3-channel RGB images, 28Ã—28 resolution, 9 classes of tissue types
ğŸ”­ Future Work

Add adversarial training as a defense mechanism
Implement stronger attacks like PGD and CW
Use Grad-CAM to visualize vulnerable regions
Evaluate robustness across different datasets

## ğŸ§‘â€ğŸ’» Author

Rohan Tiwari

